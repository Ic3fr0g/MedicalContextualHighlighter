{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Import Data\n",
    "- pos_cls is the medical data\n",
    "- neg_cls is the non-medical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
<<<<<<< Updated upstream
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
=======
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "DATA_DIR = './data/'\n",
    "MEDICAL_CSV = DATA_DIR + 'icd_10_2017.csv'\n",
    "NON_MEDICAL = DATA_DIR + 'big.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
<<<<<<< Updated upstream
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
=======
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93830 lines in pos_cls data.\n"
     ]
    }
   ],
   "source": [
    "df_icd = pd.read_csv(MEDICAL_CSV, header=None, usecols=[3, 4])\n",
    "pos_cls = df_icd[4].tolist()\n",
    "print(\"%d lines in pos_cls data.\" % len(pos_cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58670 lines in neg_cls data.\n"
     ]
    }
   ],
   "source": [
    "with open(NON_MEDICAL, encoding=\"utf-8\") as file:\n",
    "    neg_cls = [_.strip() for _ in \" \".join([l.strip() for l in file]).split(\".\")]\n",
    "print(\"%d lines in neg_cls data.\" % len(neg_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "```https://github.com/shams-sam/logic-lab/blob/master/TextPreprocessing/__preprocessing.py```\n",
    "\n",
    "- using the standard code for preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
<<<<<<< Updated upstream
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
=======
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from preprocessing import text_preprocessing\n",
    "pre = partial(text_preprocessing, HYPHEN_HANDLE = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93830 lines in pos_cls data.\n",
      "58670 lines in neg_cls data.\n"
     ]
    }
   ],
   "source": [
    "pos_cls = [pre(_) for _ in pos_cls]\n",
    "neg_cls = [pre(_) for _ in neg_cls]\n",
    "print(\"%d lines in pos_cls data.\" % len(pos_cls))\n",
    "print(\"%d lines in neg_cls data.\" % len(neg_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
=======
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
=======
   "execution_count": 8,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "NGRAM = 2"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
=======
   "execution_count": 9,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "data_tokenizer = Tokenizer()\n",
    "data_tokenizer.fit_on_texts(pos_cls + neg_cls)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
=======
   "execution_count": 10,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "data_index = {v: k for k, v in data_tokenizer.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
=======
   "execution_count": 11,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "pos_seq = data_tokenizer.texts_to_sequences(pos_cls)\n",
    "neg_seq = data_tokenizer.texts_to_sequences(neg_cls)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
=======
   "execution_count": 12,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "padding = [0] * (NGRAM-1)\n",
    "pos_seq = [padding + _ + padding for _ in pos_seq]\n",
    "neg_seq = [padding + _ + padding for _ in neg_seq]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
=======
   "execution_count": 13,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "cls_val = 0\n",
    "for _ in [neg_seq, pos_seq]:\n",
    "    for __ in _:\n",
    "        for idx in range(0, len(__)-NGRAM+1):\n",
    "            X.append(__[idx: idx+NGRAM])\n",
    "            y.append(cls_val)\n",
    "    cls_val += 1\n",
    "assert len(X) == len(y)\n",
    "num_pos_cls = len([_ for _ in y if _ == 1])\n",
    "num_neg_cls = len([_ for _ in y if _ == 0])\n",
    "assert num_pos_cls + num_neg_cls == len(y)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
=======
   "execution_count": 14,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2197293 training data available.\n",
      "1027741 positive data available.\n",
      "1169552 negative data available.\n"
     ]
    }
   ],
   "source": [
    "print(\"%d training data available.\" % len(X))\n",
    "print(\"%d positive data available.\" % num_pos_cls)\n",
    "print(\"%d negative data available.\" % num_neg_cls)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
=======
   "execution_count": 15,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape X: 2197293 rows, 2 columns\n",
      "shape y: 2197293 rows\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(\"shape X: %d rows, %d columns\" % X.shape)\n",
    "print(\"shape y: %d rows\" % y.shape)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
=======
   "execution_count": 16,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y), y)\n",
    "print('number of classes:', len(class_weights))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
=======
   "execution_count": 17,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538105 in train set.\n",
      "659188 in test set.\n"
     ]
    }
   ],
   "source": [
    "shuffle_split = StratifiedShuffleSplit(n_splits=3, test_size=0.3, random_state=0)\n",
    "shuffle_split.get_n_splits(X, y)\n",
    "for train_index, test_index in shuffle_split.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "assert len(X_train) == len(y_train)\n",
    "assert len(X_test) == len(y_test)\n",
    "print(\"%d in train set.\" % len(y_train))\n",
    "print(\"%d in test set.\" % len(y_test))\n",
    "weight_val = np.ones(len(y_test))\n",
    "for i in range(len(y_test)):\n",
    "    weight_val[i] *= class_weights[y_test[i]-1]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
=======
   "execution_count": 18,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, num_classes=len(class_weights))\n",
    "y_test = to_categorical(y_test, num_classes=len(class_weights))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
=======
   "execution_count": 19,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape X_train: 1538105 rows, 2 columns \n",
      "shape y_train: 1538105 rows, 2 columns\n",
      "shape X_test: 659188 rows, 2 columns \n",
      "shape y_test: 659188 rows, 2 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"shape X_train: %d rows, %d columns \" % X_train.shape)\n",
    "print(\"shape y_train: %d rows, %d columns\" % y_train.shape)\n",
    "print(\"shape X_test: %d rows, %d columns \" % X_test.shape)\n",
    "print(\"shape y_test: %d rows, %d columns\" % y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Word2Vec and Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
=======
   "execution_count": 20,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
=======
   "execution_count": 21,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "WORD2VEC_MODEL = '/media/enuser/db164667-d006-4ce5-a07c-8090a42e0229/wikipedia-pubmed-and-PMC-w2v/wikipedia-pubmed-and-PMC-w2v.bin'\n",
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
=======
   "execution_count": 22,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(WORD2VEC_MODEL, binary=True)\n",
    "def embedding_index(word):\n",
    "    return w2v_model.word_vec(word)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
=======
   "execution_count": 23,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 2724\n"
     ]
    }
   ],
   "source": [
    "nb_words = len(data_tokenizer.word_index)+1\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in data_tokenizer.word_index.items():\n",
    "    if word in w2v_model.vocab:\n",
    "        embedding_matrix[i] = embedding_index(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
=======
   "execution_count": 24,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "num_lstm = 234\n",
    "num_dense = 142\n",
    "rate_drop_lstm = 0.21\n",
    "rate_drop_dense = 0.24\n",
    "act = 'relu'"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
=======
   "execution_count": 25,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "import datetime\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
=======
   "execution_count": 26,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=NGRAM,\n",
    "        trainable=False)\n",
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "sequence_input = Input(shape=(NGRAM,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = lstm_layer(embedded_sequences)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(rate_drop_dense)(x)\n",
    "\n",
    "x = Dense(num_dense, activation=act)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(rate_drop_dense)(x)\n",
    "\n",
    "preds = Dense(len(class_weights), activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
=======
   "execution_count": 27,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 2, 200)            6795000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 234)               407160    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 234)               936       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 234)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 142)               33370     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 142)               568       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 142)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 286       \n",
      "=================================================================\n",
      "Total params: 7,237,320\n",
      "Trainable params: 441,568\n",
      "Non-trainable params: 6,795,752\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[sequence_input], \\\n",
    "        outputs=preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
=======
   "execution_count": 29,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_model_D20171217_T0035_234_142_0.21_0.24\n"
     ]
    }
   ],
   "source": [
    "time = datetime.datetime.now().strftime('D%Y%m%d_T%H%M')\n",
    "STAMP = 'lstm_model_' + str(time) +  '_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\n",
    "        rate_drop_dense)\n",
    "print(STAMP)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
=======
   "execution_count": 30,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1538105 samples, validate on 659188 samples\n",
      "Epoch 1/10\n",
      "1538105/1538105 [==============================] - 175s - loss: 0.1623 - acc: 0.9390 - val_loss: 0.1014 - val_acc: 0.9636\n",
      "Epoch 2/10\n",
      "1538105/1538105 [==============================] - 176s - loss: 0.1147 - acc: 0.9582 - val_loss: 0.0888 - val_acc: 0.9685\n",
      "Epoch 3/10\n",
      "1538105/1538105 [==============================] - 175s - loss: 0.1000 - acc: 0.9638 - val_loss: 0.0793 - val_acc: 0.9723\n",
      "Epoch 4/10\n",
      "1538105/1538105 [==============================] - 176s - loss: 0.0912 - acc: 0.9673 - val_loss: 0.0741 - val_acc: 0.9741\n",
      "Epoch 5/10\n",
      "1538105/1538105 [==============================] - 177s - loss: 0.0853 - acc: 0.9698 - val_loss: 0.0713 - val_acc: 0.9752\n",
      "Epoch 6/10\n",
      "1538105/1538105 [==============================] - 175s - loss: 0.0801 - acc: 0.9718 - val_loss: 0.0654 - val_acc: 0.9770\n",
      "Epoch 7/10\n",
      "1538105/1538105 [==============================] - 176s - loss: 0.0764 - acc: 0.9732 - val_loss: 0.0645 - val_acc: 0.9779\n",
      "Epoch 8/10\n",
      "1538105/1538105 [==============================] - 177s - loss: 0.0737 - acc: 0.9741 - val_loss: 0.0622 - val_acc: 0.9787\n",
      "Epoch 9/10\n",
      "1538105/1538105 [==============================] - 179s - loss: 0.0709 - acc: 0.9753 - val_loss: 0.0601 - val_acc: 0.9793\n",
      "Epoch 10/10\n",
      "1538105/1538105 [==============================] - 176s - loss: 0.0690 - acc: 0.9759 - val_loss: 0.0595 - val_acc: 0.9797\n"
     ]
    }
   ],
   "source": [
    "early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "try:\n",
    "    hist = model.fit([X_train], y_train, \\\n",
    "        validation_data=([X_test], y_test, weight_val), \\\n",
    "        epochs=10, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weights, callbacks=[early_stopping, model_checkpoint])\n",
    "except:\n",
    "    print(\"Training Stopped Manually.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_prediction_bigram(seq, verbose = False):\n",
    "    category = model.predict([seq])\n",
    "    cat = category.argmax()\n",
    "    if verbose:\n",
    "        print(category.argsort()[0][::-1])\n",
    "        print(category[0][category.argsort()[0][::-1]])\n",
    "    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_tokenizer.word_index['colonel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "get_prediction_bigram(np.atleast_2d([273, 1378]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
